---
title: "ESS 575: Markov Chain Monte Carlo Lab - Gibbs Sampling"
author: "Team England" 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: false
    toc_depth: 3
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \captionsetup[figure]{labelformat=empty}
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding){ 
    out_dir <- '../';
    rmarkdown::render(inputFile, encoding = encoding, output_file=file.path(dirname(inputFile), out_dir, 'MCMCLab_England.pdf')) 
  })
---

Team England:

  - Caroline Blommel
  - Carolyn Coyle
  - Bryn Crosby
  - George Woolsey
  
cblommel@mail.colostate.edu, carolynm@mail.colostate.edu, brcrosby@rams.colostate.edu, george.woolsey@colostate.edu


```{r setup, include=FALSE}
## load packages
library(tidyverse)
library(lubridate)
library(viridis)
library(scales)
library(latex2exp)
library(cowplot)
library(actuar)

# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  , fig.height = 5
  , fig.width = 7
)
```

# Motivation

This problem challenges you to understand how the Markov chain Monte Carlo algorithm takes a multivariate joint distribution and breaks it into a series of univariate, marginal distributions that can be approximated one at a time. There are only two unknowns in this problem, but the same principles and approach would apply if there were two hundred. The accompanying document, [MCMCMath1.pdf](https://nthobbs50.github.io/ESS575/content/labs/MCMCMath1.pdf), describes the math that stands behind the coding that you will do here. You should study this thoroughly before proceeding.



# Problem

You will write code using conjugate relationships, also known as Gibbs updates, to draw samples from marginal posterior distributions of a mean and variance.

## 1. 

Set the seed for random numbers = 10 in R with `set.seed(10)`. 

```{r}
# set seed
set.seed(10)
```

## 2. 

Load the `actuar` library, which contains functions for inverse gamma distributions needed in step 4. 

```{r}
library(actuar)
```

## 3. 

Simulate `100` data points from a normal distribution with mean $\theta = 100$ and variance $\varsigma^{2} = 25$ (where). Call the data set `y`. Be careful here. R requires the standard deviation, not the variance, as a parameter. You will use these "fake" data to verify the Gibbs sampler you will write below. Simulating data is always a good way to test methods. Your method should be able to recover the generating parameters given a sufficiently large number of simulated observations.

The variance in the likelihood ($\varsigma^{2}$) is the variance of the distribution of the observations ($y_i$), not the variance of the distribution of the mean of the observations ($\theta$).

```{r}
# set parameters
n <- 100
varsigma_sq <- 25
theta <- 100
# draw random sample
y <- rnorm(n = n, mean = theta, sd = sqrt(varsigma_sq))
```


## 4. 

I have saved you some time by writing a function called `draw_mean` that makes draws from the marginal posterior distributions for $\theta$ using a normal-normal conjugate relationship where the variance is assumed to be known. It is vital that you study the [MCMCMath1.pdf](https://nthobbs50.github.io/ESS575/content/labs/MCMCMath1.pdf) notes relative to this function.

```{r}
# normal likelihood with normal prior conjugate for mean, assuming variance is known
# mu_0 is prior mean
# sigma.sq_0 is prior variance of mean
# varsigma.sq is known variance of data

draw_mean = function(mu_0, sigma.sq_0, varsigma.sq, y){
    mu_1 =((mu_0 / sigma.sq_0 + sum(y)/varsigma.sq)) / (1/sigma.sq_0 + length(y) / varsigma.sq)
    sigma.sq_1 = 1/(1 / sigma.sq_0 + length(y) / varsigma.sq)
    z = rnorm(1, mu_1, sqrt(sigma.sq_1))
    param = list(z = z, mu_1 = mu_1, sigma.sq_1 = sigma.sq_1)
    return(param)
}
```

## 5. 

I have also provided a function called `draw_var` that makes draws from the marginal posterior distribution for $\sigma^{2}$ using a inverse gamma-normal conjugate relationship where the mean is assumed to be known. Study this function relative to the [MCMCMath1.pdf](https://nthobbs50.github.io/ESS575/content/labs/MCMCMath1.pdf) handout.

```{r}
# normal likelihood with gamma prior conjugate relationship for variance, assuming mean is known
# alpha_0 is parameter of prior for variance
# beta_0 is parameter of prior for variance
# Note that this uses scale parameterization for inverse gamma

draw_var = function(alpha_0, beta_0, theta, y){
    alpha_1 = alpha_0 + length(y) / 2
    beta_1 = beta_0 + sum((y - theta)^2) / 2
    z = actuar::rinvgamma(1, alpha_1, scale = beta_1)
    param = list(z = z, alpha_1 = alpha_1, beta_1 = beta_1)
    return(param)
}
```

## 6. 

Check the functions by simulating a large number of data points from a normal distribution using a mean and variance of your own choosing. Store the data points in a vector called `y_check`. Assume flat priors for the mean and the variance. A vague prior for the inverse gamma has parameters $\alpha_{0} = 0.001$ and $\beta_{0} = 0.001$.

```{r}
# set parameters
n_check <- 10000
varsigma_sq_check <- 11
theta_check <- 23
alpha_0 <- 0.001
beta_0 <- 0.001
# draw random sample
y_check <- rnorm(n = n_check, mean = theta_check, sd = sqrt(varsigma_sq_check))
# set up vector to hold simulation results
variance_check <- numeric(n_check)
mean_check <- numeric(n_check)
# check draw functions
for(i in 1:n_check){
  # draw_mean
  mean_check[i] <- draw_mean(
      mu_0 = 0 # assume flat prior
      , sigma.sq_0 = n_check # assume flat prior (the variance is the full range of data 0 to n)
      , varsigma.sq = varsigma_sq_check
      , y = y_check
    )$z # this returns only the random variable from the draw
  # draw_var
  variance_check[i] <- draw_var(
      alpha_0 = alpha_0
      , beta_0 = beta_0
      , theta = theta_check
      , y = y_check
    )$z # this returns only the random variable from the draw
} 
```

\textcolor{violet}{The mean of the draws from the marginal posterior distributions for $\theta$ using a normal-normal conjugate relationship where the variance is assumed to be known based on a simulated normal distribution with mean = `r theta_check` and a variance = `r varsigma_sq_check` is:} **\textcolor{violet}{`r scales::comma(mean(mean_check), accuracy = .1)`}**

\textcolor{violet}{The mean of the draws from the marginal posterior distribution for $\sigma^{2}$ using a inverse gamma-normal conjugate relationship where the mean is assumed to be known based on a simulated normal distribution with mean = `r theta_check` and a variance = `r varsigma_sq_check` is:} **\textcolor{violet}{`r scales::comma(mean(variance_check), accuracy = .1)`}**

```{r, echo=FALSE}
remove(list = ls()[grep("_check",ls())])
```

# Writing a sampler

Now execute these steps:

## 1.

Set up a matrix for storing samples from the posterior distribution of the mean. The number of rows should equal the number of chains (3) and number of columns should equal the number of iterations (10,000). Do the same thing for storing samples from the posterior distribution of the variance.

```{r}
# set parameters
n_iterations <- 10000
n_chains <- 3
# set up matrix for storing samples
posterior_mean <- matrix(nrow = n_chains, ncol = n_iterations)
posterior_variance <- matrix(nrow = n_chains, ncol = n_iterations)
```


## 2.

Assign initial values to the first column of each matrix, a different value for each of the chains. These can be virtually any value within the support of the random variable, but it would be fine for this exercise to use values not terribly far away from to those you used to simulate the data, reflecting some prior knowledge. You might try varying these later to show that you will get the same results.

```{r}
# set initial values in first column
posterior_mean[1:3, 1] <- c(123, 111, 85)
posterior_variance[1:3, 1] <- c(33, 11, 3)
# check
posterior_mean[,1:6]
posterior_variance[,1:6]
```


## 3.

Set up nested for loops to iterate from one to the total number of iterations for each of the three chains for each parameter. Use the conjugate functions `draw_mean` and `draw_var` to draw a sample from the distribution of the mean using the value of the variance at the current iteration. Then make a draw from the variance using the current value of the mean. Repeat. Assume vague priors for the mean and variance:


```{r}
# A vague prior for the inverse gamma has parameters:
alpha_0 <- 0.001
beta_0 <- 0.001
# Set up nested for loops
for(c in 1:n_chains){
  for(i in 2:n_iterations){
    # draw_mean
    posterior_mean[c, i] <- draw_mean(
        mu_0 = 0 # assume flat prior
        , sigma.sq_0 = n_iterations # assume flat prior (the variance is the full range of data 0 to n)
        , varsigma.sq = posterior_variance[c, i-1]
        , y = y
      )$z # this returns only the random variable from the draw
    # draw_var
    posterior_variance[c, i] <- draw_var(
        alpha_0 = alpha_0
        , beta_0 = beta_0
        , theta = posterior_mean[c, i-1]
        , y = y
      )$z # this returns only the random variable from the draw
  } 
}
# check
posterior_mean[,1:6]
posterior_variance[,1:6]
```

# Trace plots and plots of marginal posteriors

## 1.

Discard the first 1000 iterations as burn-in. Plot the value of the mean as a function of iteration number for each chain. This is called a trace plot.

```{r}
# Discard the first 1000 iterations as burn-in.
burnin <- 1000
# make burnin data frames
# mean
posterior_mean_burnin <- posterior_mean[,(burnin+1):n_iterations] %>% 
  as.data.frame() %>% 
  dplyr::mutate(chain = as.character(dplyr::row_number())) %>% 
  tidyr::pivot_longer(
    cols = tidyr::starts_with("V")
    , names_to = "iteration"
    , names_prefix = "V"
    , values_to = "value"
    , values_drop_na = FALSE
  ) %>% 
  dplyr::mutate(iteration = as.numeric(iteration)+burnin)
# variance
posterior_variance_burnin <- posterior_variance[,(burnin+1):n_iterations] %>% 
  as.data.frame() %>% 
  dplyr::mutate(chain = as.character(dplyr::row_number())) %>% 
  tidyr::pivot_longer(
    cols = tidyr::starts_with("V")
    , names_to = "iteration"
    , names_prefix = "V"
    , values_to = "value"
    , values_drop_na = FALSE
  ) %>% 
  dplyr::mutate(iteration = as.numeric(iteration)+burnin)
# Plot the value of the mean as a function of iteration number for each chain
ggplot(
    data = posterior_mean_burnin
    , mapping = aes(x = iteration, y = value, color = chain)
  ) +
  geom_line(lwd = 1) +
  scale_color_viridis_d(alpha = 0.7, option = "viridis") +
  xlab("Iteration number") + 
  ylab(latex2exp::TeX("$\\theta$")) +
  theme_bw() +
  theme(
    legend.position = "top"
    , legend.direction = "horizontal"
  ) +
  guides(color = guide_legend(override.aes = list(size = 5)))
```


## 2.

Make a histogram of the samples of the mean retained after burn-in including all chains. Put a vertical line on the plot showing the generating value.

```{r}
# Make a histogram of the samples of the mean retained after burn-in
ggplot(
    data = posterior_mean_burnin
    , mapping = aes(x = value)
  ) +
  geom_histogram(
    aes(y = ..density..)
    , bins = 100
    , fill = "navy"
    , alpha = 0.8
    , color = "gray25"
  ) +
  geom_density(
    aes(y = ..density..)
    , linetype = 2
    , lwd = 1.2
    , color = "orangered"
  ) +
  geom_vline(xintercept = theta, color = "gray50", linetype = 2, lwd = 1) +
  annotate("text", x = theta*1.0005, y = 0.5, parse = TRUE, label = "theta", color = "black") +
  xlab(latex2exp::TeX("$\\theta$ sample value")) +
  ylab("Density") +
  theme_bw()
```


## 3.

Repeat steps 1-2 for the variance.

```{r}
# Plot the value of the variance as a function of iteration number for each chain
ggplot(
    data = posterior_variance_burnin
    , mapping = aes(x = iteration, y = value, color = chain)
  ) +
  geom_line(lwd = 1) +
  scale_color_viridis_d(alpha = 0.7, option = "plasma") +
  xlab("Iteration number") + 
  ylab(latex2exp::TeX("$\\varsigma^2$")) +
  theme_bw() +
  theme(
    legend.position = "top"
    , legend.direction = "horizontal"
  ) +
  guides(color = guide_legend(override.aes = list(size = 5)))
```


```{r}
# Make a histogram of the samples of the variance retained after burn-in
ggplot(
    data = posterior_variance_burnin
    , mapping = aes(x = value)
  ) +
  geom_histogram(
    aes(y = ..density..)
    , bins = 100
    , fill = "steelblue"
    , alpha = 0.8
    , color = "gray25"
  ) +
  geom_density(
    aes(y = ..density..)
    , linetype = 2
    , lwd = 1.2
    , color = "orangered"
  ) +
  geom_vline(xintercept = varsigma_sq, color = "gray50", linetype = 2, lwd = 1) +
  annotate("text", x = varsigma_sq*1.015, y = 0.12, parse = TRUE, label = "varsigma^2", color = "black") +
  xlab(latex2exp::TeX("$\\varsigma^2$ sample value")) +
  ylab("Density") +
  theme_bw()
```

## 4.

For both $\theta$ and $\varsigma^2$, calculate the mean of all the chains combined and its standard deviation. Interpret these quantities.

## 5.

Compare the standard deviation of the posterior distribution of $\theta$ with an approximation using the standard deviation of the data divided by the square root of the sample size. What is this approximation called in the frequentist world?

## 6.

Vary the number of values in the simulated data set, e.g., n = 10, 100, 1,000, 10,000. We do not exactly recover the generating values of $\theta$ and $\varsigma^2$ when n is small. Why? The mean of the marginal posterior distribution of the variance is further away from its generating value than the mean is. Why? Try different values for set seed with n = 100 and interpret the effect of changing the random number sequence.

## 7.

Make the burnin = 1 instead of 1000. Does this change your results? Why or why not?

## 8.

Reverse the order of the conjugate functions in step 3 of the Writing a Sampler section so that the variance is drawn first followed by the mean. (Be careful, this involves a bit more than simply reversing the order of the functions in the loop). Does this reordering have an effect on the posteriors? Why or why not?
